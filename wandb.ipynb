{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e632309",
   "metadata": {},
   "source": [
    "# Weights & Biases (W&B) Guide: Experiment Tracking and Model Registry\n",
    "\n",
    "## What is Weights & Biases?\n",
    "\n",
    "Weights & Biases (W&B) is a powerful tool for managing machine learning workflows. It offers:\n",
    "\n",
    "- **Experiment tracking**: Automatically logs metrics, hyperparameters, system info, and more.\n",
    "- **Model versioning and registry**: Save, version, and manage your models across different stages.\n",
    "- **Interactive dashboards**: Visualize and compare performance metrics in real time.\n",
    "- **Collaboration tools**: Share results and insights easily with your team.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Use W&B?\n",
    "\n",
    "As your ML projects grow in complexity, it's crucial to keep track of what you've trained, how, and why. W&B helps ensure your work is:\n",
    "\n",
    "| Feature                  | Benefit                                                                 |\n",
    "|--------------------------|-------------------------------------------------------------------------|\n",
    "| Auto-logging             | Seamlessly logs training metrics, parameters, gradients, and system stats |\n",
    "| Reproducible             | Keeps a snapshot of every run: code, configs, results                   |\n",
    "| Dashboard visualization  | Plots metrics like loss, accuracy, learning rate over time              |\n",
    "| Experiment management    | Helps group, filter, and compare training runs                          |\n",
    "| Model registry           | Store, tag, and manage multiple model versions                          |\n",
    "| Team collaboration       | Share projects and results easily with team members                     |\n",
    "\n",
    "---\n",
    "\n",
    "## Scope of This Guide\n",
    "\n",
    "This notebook focuses on the two most useful parts of W&B for most machine learning workflows:\n",
    "\n",
    "1. **Experiment Tracking**\n",
    "   - Initializing W&B in a script or notebook\n",
    "   - Logging metrics, configs, and artifacts\n",
    "   - Organizing and comparing runs\n",
    "\n",
    "2. **Model Registry**\n",
    "   - Saving models as W&B artifacts\n",
    "   - Managing model versions\n",
    "   - Promoting models to different stages (e.g., Staging → Production)\n",
    "\n",
    "---\n",
    "\n",
    "> Note: W&B works with most ML frameworks including PyTorch, TensorFlow, Hugging Face Transformers, Scikit-learn, and more.\n",
    "\n",
    "---\n",
    "\n",
    "Next Step: [Setup & Installation](#)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93ad9e9",
   "metadata": {},
   "source": [
    "### Install the wandb library and log in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf1b6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cd4e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# wandb.login(key=<your-api-key>)  # Uncomment the line above and replace <your-api-key> with your actual API key if needed\n",
    "wandb.login()\n",
    "e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642fa2ca",
   "metadata": {},
   "source": [
    "### Projects\n",
    "\n",
    "A **project** in W&B is the central hub for organizing and analyzing your machine learning experiments. It allows you to:\n",
    "\n",
    "- Compare different versions of your models  \n",
    "- Explore results in a personal scratch workspace  \n",
    "- Track and download artifacts  \n",
    "- Set up automations and sweeps  \n",
    "- Save insights and visualizations in shareable reports  \n",
    "\n",
    "Each project contains the following key tabs:\n",
    "\n",
    "- **Overview**: A snapshot of key metrics, recent activity, and project health  \n",
    "- **Workspace**: A customizable visualization sandbox for charts and filters  \n",
    "- **Runs**: A sortable, filterable table of all runs in the project  \n",
    "- **Automations**: Automated workflows triggered by events or conditions  \n",
    "- **Sweeps**: Tools for hyperparameter tuning and exploration  \n",
    "- **Reports**: Saved collections of runs, graphs, and notes for easy sharing  \n",
    "- **Artifacts**: Versioned files and models associated with each run  \n",
    "\n",
    "#### Creating a Project Programmatically\n",
    "\n",
    "To create or log to a project from your script, pass the `project` argument to `wandb.init()`. If the project does not already exist, it will be created automatically under the specified entity (user or team account).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90f6809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb \n",
    "\n",
    "with wandb.init(\n",
    "    # entity=\"<entity>\",  # Optional, defaults to your account if not specified\n",
    "    project=\"my-firsr-project\") as run: run.log({\"accuracy\": .95}) \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f8e89a",
   "metadata": {},
   "source": [
    "### Runs\n",
    "\n",
    "A **Run** in Weights & Biases represents a single execution of your training script or notebook. It logs all relevant metadata — metrics, hyperparameters, system stats, artifacts, and more.\n",
    "\n",
    "You can think of a run as a snapshot of one experiment.\n",
    "\n",
    "---\n",
    "\n",
    "#### Key Concepts\n",
    "\n",
    "- **Starting a run**: You initialize tracking by calling `wandb.init()`.\n",
    "- **Run name**: Custom name you assign to the run (otherwise W&B generates one randomly).\n",
    "- **Run ID**: A stable, unique ID (UUID) for the run — used for resuming or referencing.\n",
    "- **Finishing a run**: Call `wandb.finish()` to finalize logging.\n",
    "- **Tags**: You can add tags to runs for easier filtering and organization.\n",
    "- **Notes**: Add descriptive notes to runs for context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7af88be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import wandb\n",
    "\n",
    "# Initialize a run\n",
    "run = wandb.init(\n",
    "    project=\"my-first-project\",\n",
    "    name=\"baseline-lr-0.01\",          # Optional: custom name\n",
    "    # id=\"abc123\",                    # Optional: specify a run ID otherwise one will be generated\n",
    "    tags=[\"baseline\", \"lr-0.01\"],  # Optional: add tags to the run\n",
    "    notes=\"This is a baseline run with learning rate 0.01\",  # Optional: add notes to the run\n",
    ")\n",
    "\n",
    "# Log a metric\n",
    "for epoch in range(10):\n",
    "    wandb.log({\"loss\": 0.5 - 0.05 * epoch, \"epoch\": epoch}) # simulated loss value\n",
    "\n",
    "print(\"Run ID:\", run.id)\n",
    "print(\"Run Name:\", run.name)\n",
    "print(\"Run Path:\", run.path)  # Format: entity/project/run_id\n",
    "\n",
    "# Finish the run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e6e519",
   "metadata": {},
   "source": [
    "### Logging Everything in W&B\n",
    "\n",
    "This code block demonstrates how to log:\n",
    "\n",
    "- Scalar metrics (loss, accuracy)\n",
    "- Hyperparameters (`config`)\n",
    "- Images\n",
    "- Text (HTML)\n",
    "- Audio (synthetic sine wave)\n",
    "- Dataset artifact (CSV file)\n",
    "- Model artifact (PyTorch .pt file)\n",
    "- Tabular data (W&B Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f095a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in c:\\users\\moiz.asghar\\desktop\\atom\\wandb-guide\\wandb\\lib\\site-packages (0.21.0)\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.10.3-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Collecting torch\n",
      "  Using cached torch-2.7.1-cp311-cp311-win_amd64.whl.metadata (28 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.7.1-cp311-cp311-win_amd64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in c:\\users\\moiz.asghar\\desktop\\atom\\wandb-guide\\wandb\\lib\\site-packages (from wandb) (8.2.1)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\\users\\moiz.asghar\\desktop\\atom\\wandb-guide\\wandb\\lib\\site-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: packaging in c:\\users\\moiz.asghar\\desktop\\atom\\wandb-guide\\wandb\\lib\\site-packages (from wandb) (25.0)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\moiz.asghar\\desktop\\atom\\wandb-guide\\wandb\\lib\\site-packages (from wandb) (4.3.8)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in c:\\users\\moiz.asghar\\desktop\\atom\\wandb-guide\\wandb\\lib\\site-packages (from wandb) (6.31.1)\n",
      "Requirement already satisfied: pydantic<3 in c:\\users\\moiz.asghar\\desktop\\atom\\wandb-guide\\wandb\\lib\\site-packages (from wandb) (2.11.7)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\moiz.asghar\\desktop\\atom\\wandb-guide\\wandb\\lib\\site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\moiz.asghar\\desktop\\atom\\wandb-guide\\wandb\\lib\\site-packages (from wandb) (2.32.4)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in c:\\users\\moiz.asghar\\desktop\\atom\\wandb-guide\\wandb\\lib\\site-packages (from wandb) (2.32.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in c:\\users\\moiz.asghar\\desktop\\atom\\wandb-guide\\wandb\\lib\\site-packages (from wandb) (4.14.1)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.3.2-cp311-cp311-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.58.5-cp311-cp311-win_amd64.whl.metadata (109 kB)\n",
      "     ---------------------------------------- 0.0/109.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/109.0 kB ? eta -:--:--\n",
      "     ------- -------------------------------- 20.5/109.0 kB ? eta -:--:--\n",
      "     ---------- -------------------------- 30.7/109.0 kB 435.7 kB/s eta 0:00:01\n",
      "     ---------- -------------------------- 30.7/109.0 kB 435.7 kB/s eta 0:00:01\n",
      "     ------------------------------- ----- 92.2/109.0 kB 581.0 kB/s eta 0:00:01\n",
      "     ------------------------------------ 109.0/109.0 kB 525.6 kB/s eta 0:00:00\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.8-cp311-cp311-win_amd64.whl.metadata (6.3 kB)\n",
      "Collecting numpy>=1.23 (from matplotlib)\n",
      "  Using cached numpy-2.3.1-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-11.3.0-cp311-cp311-win_amd64.whl.metadata (9.2 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\moiz.asghar\\desktop\\atom\\wandb-guide\\wandb\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\moiz.asghar\\desktop\\atom\\wandb-guide\\wandb\\lib\\site-packages (from click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\moiz.asghar\\desktop\\atom\\wandb-guide\\wandb\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\moiz.asghar\\desktop\\atom\\wandb-guide\\wandb\\lib\\site-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\moiz.asghar\\desktop\\atom\\wandb-guide\\wandb\\lib\\site-packages (from pydantic<3->wandb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\moiz.asghar\\desktop\\atom\\wandb-guide\\wandb\\lib\\site-packages (from pydantic<3->wandb) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\moiz.asghar\\desktop\\atom\\wandb-guide\\wandb\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\moiz.asghar\\desktop\\atom\\wandb-guide\\wandb\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\moiz.asghar\\desktop\\atom\\wandb-guide\\wandb\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\moiz.asghar\\desktop\\atom\\wandb-guide\\wandb\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\moiz.asghar\\desktop\\atom\\wandb-guide\\wandb\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2025.7.9)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached MarkupSafe-3.0.2-cp311-cp311-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\moiz.asghar\\desktop\\atom\\wandb-guide\\wandb\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Using cached matplotlib-3.10.3-cp311-cp311-win_amd64.whl (8.1 MB)\n",
      "Using cached torch-2.7.1-cp311-cp311-win_amd64.whl (216.1 MB)\n",
      "Downloading torchaudio-2.7.1-cp311-cp311-win_amd64.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/2.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/2.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/2.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/2.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/2.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.4/2.5 MB 1.6 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 0.6/2.5 MB 1.7 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 1.1/2.5 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.5/2.5 MB 3.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 2.1/2.5 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 MB 4.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.5/2.5 MB 4.7 MB/s eta 0:00:00\n",
      "Using cached contourpy-1.3.2-cp311-cp311-win_amd64.whl (222 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.58.5-cp311-cp311-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.6/2.2 MB 33.9 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.0/2.2 MB 10.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.7/2.2 MB 11.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.2/2.2 MB 11.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 11.0 MB/s eta 0:00:00\n",
      "Using cached kiwisolver-1.4.8-cp311-cp311-win_amd64.whl (71 kB)\n",
      "Using cached numpy-2.3.1-cp311-cp311-win_amd64.whl (13.0 MB)\n",
      "Downloading pillow-11.3.0-cp311-cp311-win_amd64.whl (7.0 MB)\n",
      "   ---------------------------------------- 0.0/7.0 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.6/7.0 MB 17.5 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 1.1/7.0 MB 11.5 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 1.4/7.0 MB 11.4 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 1.9/7.0 MB 11.3 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 2.4/7.0 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 2.8/7.0 MB 10.5 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 3.3/7.0 MB 10.5 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 3.8/7.0 MB 11.0 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 4.2/7.0 MB 10.6 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 4.6/7.0 MB 10.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 5.1/7.0 MB 10.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.7/7.0 MB 10.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 6.2/7.0 MB 10.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.6/7.0 MB 10.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.0/7.0 MB 10.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.0/7.0 MB 9.7 MB/s eta 0:00:00\n",
      "Using cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "   ---------------------------------------- 0.0/199.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 199.1/199.1 kB 5.9 MB/s eta 0:00:00\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Using cached MarkupSafe-3.0.2-cp311-cp311-win_amd64.whl (15 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, pyparsing, pillow, numpy, networkx, MarkupSafe, kiwisolver, fsspec, fonttools, filelock, cycler, jinja2, contourpy, torch, matplotlib, torchaudio\n",
      "Successfully installed MarkupSafe-3.0.2 contourpy-1.3.2 cycler-0.12.1 filelock-3.18.0 fonttools-4.58.5 fsspec-2025.5.1 jinja2-3.1.6 kiwisolver-1.4.8 matplotlib-3.10.3 mpmath-1.3.0 networkx-3.5 numpy-2.3.1 pillow-11.3.0 pyparsing-3.2.3 sympy-1.14.0 torch-2.7.1 torchaudio-2.7.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install wandb torch numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fa02245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2ddd9b",
   "metadata": {},
   "source": [
    "### Step 1: Initialize a W&B Run\n",
    "\n",
    "We start by initializing a W&B run using `wandb.init()`. Here we specify:\n",
    "\n",
    "- `project`: Name of the project the run belongs to\n",
    "- `name`: Human-readable name of the run (optional)\n",
    "- `config`: Dictionary of hyperparameters and settings to log\n",
    "\n",
    "The returned `run` object gives access to the run's metadata like ID and name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d65252f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmoizasghar\u001b[0m (\u001b[33mmoizasghar-afiniti\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.7s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\moiz.asghar\\Desktop\\Atom\\wandb-guide\\wandb\\run-20250714_163443-p6ccx3fx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/moizasghar-afiniti/wandb-complete-logging-demo/runs/p6ccx3fx' target=\"_blank\">full-logging-example-1</a></strong> to <a href='https://wandb.ai/moizasghar-afiniti/wandb-complete-logging-demo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/moizasghar-afiniti/wandb-complete-logging-demo' target=\"_blank\">https://wandb.ai/moizasghar-afiniti/wandb-complete-logging-demo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/moizasghar-afiniti/wandb-complete-logging-demo/runs/p6ccx3fx' target=\"_blank\">https://wandb.ai/moizasghar-afiniti/wandb-complete-logging-demo/runs/p6ccx3fx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize W&B run\n",
    "run = wandb.init(\n",
    "    project=\"wandb-complete-logging-demo\",\n",
    "    name=\"full-logging-example-1\",\n",
    "    config={\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"batch_size\": 32,\n",
    "        \"optimizer\": \"adam\",\n",
    "        \"epochs\": 5\n",
    "    },\n",
    "    resume=\"allow\",  # Resume from the last run if it exists\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a78d05",
   "metadata": {},
   "source": [
    "### Step 2: Log Scalar Metrics\n",
    "\n",
    "We use `wandb.log()` to log metrics like loss and accuracy over time. These are typically recorded during training or evaluation.\n",
    "\n",
    "Each log entry can include multiple key-value pairs and is automatically time-stamped and plotted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf70b1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Log metrics\n",
    "for epoch in range(5):\n",
    "    loss = 0.5 - 0.05 * epoch\n",
    "    accuracy = 0.7 + 0.06 * epoch\n",
    "    wandb.log({\n",
    "        \"train/loss\": loss,\n",
    "        \"val/accuracy\": accuracy,\n",
    "        \"epoch\": epoch\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20b282b",
   "metadata": {},
   "source": [
    "### Step 3: Log an Image\n",
    "\n",
    "You can log images to visually inspect inputs, predictions, or intermediate results. W&B supports NumPy arrays, PIL images, and more.\n",
    "\n",
    "Here, we log a random RGB image using `wandb.Image()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97c995e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Log image\n",
    "img = np.random.randint(0, 255, (28, 28, 3), dtype=np.uint8)\n",
    "wandb.log({\"random_image\": wandb.Image(img, caption=\"Random RGB Image\")})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4b7fa2",
   "metadata": {},
   "source": [
    "### Step 4: Log Text/HTML Output\n",
    "\n",
    "W&B allows logging HTML, markdown, or plain text using `wandb.Html()` or just a string.\n",
    "\n",
    "This is useful for saving model outputs, sample predictions, or logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "612b203e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Log text\n",
    "wandb.log({\"sample_text\": wandb.Html(\"<b>Sample output:</b> This is a test log\")})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f1e1fe",
   "metadata": {},
   "source": [
    "### Step 5: Log Audio\n",
    "\n",
    "You can log audio waveforms using `wandb.Audio()`, which is great for speech models.\n",
    "\n",
    "In this example, we generate a 440 Hz sine wave and log it as an audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eaf3bc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting soundfile\n",
      "  Downloading soundfile-0.13.1-py2.py3-none-win_amd64.whl.metadata (16 kB)\n",
      "Collecting cffi>=1.0 (from soundfile)\n",
      "  Downloading cffi-1.17.1-cp311-cp311-win_amd64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\moiz.asghar\\desktop\\atom\\wandb-guide\\wandb\\lib\\site-packages (from soundfile) (2.3.1)\n",
      "Collecting pycparser (from cffi>=1.0->soundfile)\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Downloading soundfile-0.13.1-py2.py3-none-win_amd64.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.1/1.0 MB 812.7 kB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 0.1/1.0 MB 1.0 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.3/1.0 MB 1.7 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 0.5/1.0 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.0/1.0 MB 4.3 MB/s eta 0:00:00\n",
      "Downloading cffi-1.17.1-cp311-cp311-win_amd64.whl (181 kB)\n",
      "   ---------------------------------------- 0.0/181.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 181.4/181.4 kB 10.7 MB/s eta 0:00:00\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: pycparser, cffi, soundfile\n",
      "Successfully installed cffi-1.17.1 pycparser-2.22 soundfile-0.13.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3fb47f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Log audio (sine wave)\n",
    "sample_rate = 16000\n",
    "duration = 1  # seconds\n",
    "t = torch.linspace(0, duration, int(sample_rate * duration))\n",
    "sine_wave = 0.5 * torch.sin(2 * np.pi * 440 * t)\n",
    "wandb.log({\"sample_audio\": wandb.Audio(sine_wave.numpy(), sample_rate=sample_rate)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151a52e7",
   "metadata": {},
   "source": [
    "### Step 3: Log a Dataset Artifact\n",
    "\n",
    "Artifacts in W&B are versioned objects like datasets or checkpoints. You can add local files to an artifact and log it.\n",
    "\n",
    "Here, we create a simple CSV file and log it as a dataset artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2b52b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Artifact tiny-dataset>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"sample_data.csv\", \"w\") as f:\n",
    "    f.write(\"epoch,accuracy\\n\")\n",
    "    for i in range(5):\n",
    "        f.write(f\"{i},{0.7 + 0.06 * i}\\n\")\n",
    "\n",
    "dataset_artifact = wandb.Artifact(name=\"tiny-dataset\", type=\"dataset\")\n",
    "dataset_artifact.add_file(\"sample_data.csv\")\n",
    "run.log_artifact(dataset_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0713f9",
   "metadata": {},
   "source": [
    "### Step 4: Log a Model Artifact\n",
    "\n",
    "Model files like `.pt`, `.h5`, or `.pkl` can be logged as `model` artifacts. This helps version control your model checkpoints.\n",
    "\n",
    "We simulate this by saving a dummy PyTorch model dictionary and logging it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78b87e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Artifact dummy-model>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. Log a model artifact\n",
    "torch.save({\"model_state\": \"dummy_weights\"}, \"model.pt\")\n",
    "model_artifact = wandb.Artifact(name=\"dummy-model\", type=\"model\")\n",
    "model_artifact.add_file(\"model.pt\")\n",
    "run.log_artifact(model_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f375fcaf",
   "metadata": {},
   "source": [
    "### Step 8: Log a Table\n",
    "\n",
    "W&B Tables allow you to log structured data (rows + columns), which can be visualized and explored interactively.\n",
    "\n",
    "We create a small table with training metrics over epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "753a3234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Log a table\n",
    "table = wandb.Table(columns=[\"epoch\", \"loss\", \"accuracy\"])\n",
    "for i in range(5):\n",
    "    table.add_data(i, 0.5 - 0.05 * i, 0.7 + 0.06 * i)\n",
    "wandb.log({\"training_metrics\": table})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bf96ae",
   "metadata": {},
   "source": [
    "### Step 9: Register Model to the W&B Model Registry\n",
    "\n",
    "The W&B Model Registry is built on top of artifacts. To register a model:\n",
    "\n",
    "1. Log the model as a `model` artifact (already done in Step 7)\n",
    "2. Assign a version or alias (e.g., `v1`, `staging`, `production`) to the artifact\n",
    "\n",
    "You can use `.link()` to add your model to the central model registry for your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c2ba45a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Artifact QXJ0aWZhY3Q6MTg3Nzk2NTIyMw==>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wait until the artifact is fully uploaded and versioned\n",
    "model_artifact.wait()\n",
    "\n",
    "# Link the model to the registry with version alias\n",
    "run.link_artifact(\n",
    "    model_artifact, \n",
    "    target_path=\"model-registry/dummy-model\",  # registry path: <registry_name>/<model_name>\n",
    "    aliases=[\"v1\", \"staging\"]                 # you can add multiple aliases\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5661dd4f",
   "metadata": {},
   "source": [
    "### Final Step: Finish the Run\n",
    "\n",
    "Always call `wandb.finish()` at the end of your script or training loop to ensure all logs are uploaded properly and the run is finalized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0fabf32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>train/loss</td><td>█▆▅▃▁</td></tr><tr><td>val/accuracy</td><td>▁▃▅▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>train/loss</td><td>0.3</td></tr><tr><td>val/accuracy</td><td>0.94</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">full-logging-example-1</strong> at: <a href='https://wandb.ai/moizasghar-afiniti/wandb-complete-logging-demo/runs/p6ccx3fx' target=\"_blank\">https://wandb.ai/moizasghar-afiniti/wandb-complete-logging-demo/runs/p6ccx3fx</a><br> View project at: <a href='https://wandb.ai/moizasghar-afiniti/wandb-complete-logging-demo' target=\"_blank\">https://wandb.ai/moizasghar-afiniti/wandb-complete-logging-demo</a><br>Synced 5 W&B file(s), 4 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250714_163443-p6ccx3fx\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wandb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
